## System Prompt
You are an expert programmer and problem solver.
Your task is to write a correct and general policy in Python.
You may define helper functions if needed.
Return only executable Python code.

--

## Environment description:

- The environment is a grid world of the game 'StopTheFall'.
- The observation `obs` is a 2D Python NumPy array (rows × columns).
  - Do not use boolean checks such as `if obs`, `if not obs`, or `obs == []`. 
- Each cell contains exactly one of the following symbols:

  - empty
  - falling
  - red
  - static
  - advance
  - drawn

## Game Dynamics and Rules

- The game unfolds on a 2D grid representing a dynamic environment with gravity-like behavior.
- The agent occupies exactly one cell and can reposition itself by selecting grid cells as actions.
- Other objects in the environment evolve over time independently of the agent’s direct control.

### Falling Objects

- Cells marked as `falling` represent objects that move downward over time according to the environment’s internal dynamics.
- Falling objects pose a threat: if they reach certain critical regions or states without being intercepted or mitigated, the episode ends unsuccessfully.
- The agent cannot directly control falling objects but can influence outcomes by positioning itself relative to their paths.

### Hazard and Obstacle Cells

- Cells marked as `red` are hazardous.
  - If the agent enters a red cell, the episode immediately ends unsuccessfully.
  - Red cells may also indicate regions where falling objects should not be allowed to reach.
- Cells marked as `static` represent immovable obstacles or structural elements.
  - These cells block movement and falling-object traversal but do not change over time.
- Cells marked as `drawn` are part of the environment’s structure but do not have special dynamics by themselves.

### Advance Cells and Environmental Cues

- Cells marked as `advance` act as observable cues related to progression, timing, or environmental state changes.
- These cells do not correspond to explicit actions or immediate rewards but may signal when progression, survival milestones, or transitions are imminent.
- The agent may use the spatial arrangement of advance cells to infer safe timing or positioning.

### Movement and Action Semantics

- On each step, the agent selects a target cell `(row, col)` as its action.
- The environment determines whether the action results in movement, blocking, interception, or no effect.
- Selecting invalid, unreachable, or unsafe cells (including red cells) results in no beneficial outcome and may trigger failure.
- The agent cannot override gravity, collision rules, or timing mechanics; these are fully handled by the environment.

### Objective and Termination Conditions

- The objective is to survive until the end of the episode by preventing falling objects from causing failure and by avoiding red cells.
- The episode ends unsuccessfully if:
  - A falling object reaches a critical failure condition, or
  - The agent enters a red cell.
- The episode ends successfully if the agent survives through all environmental dynamics until termination.

### Information and Constraints

- The policy has no access to the environment’s internal physics, update rules, or future object trajectories.
- All decisions must be based solely on the current grid observation.
- Strategic behavior therefore relies on interpreting spatial relationships, timing cues, and object configurations visible in the grid.
- All enforcement of legality, collisions, falling behavior, and termination is handled internally by the environment.

--

## Action space:

The policy must return a tuple (row, column) corresponding to a cell in the grid.

- (row, column) must be a valid cell location within the grid bounds.
- Selecting a cell represents choosing that cell as the agent’s action.
- The environment defines how selecting a cell affects the agent and other objects.
- Selecting an invalid or unsafe cell has no effect.

--

## Task Description
Task:
Prevent falling objects from causing failure.
The agent should position itself to block or avoid falling objects and must avoid red cells.
The episode ends unsuccessfully if a falling object reaches a critical state or if the agent enters a red cell.
The episode is successful if the agent survives until the end of the episode.

--

## Expert-Derived Hints

Below is a consolidated description of the agent’s demonstrated decision-making strategy, inferred from multiple expert trajectories. The statements summarize consistent behaviors observed across rollouts and are intended to guide policy synthesis, not to prescribe explicit rules or primitives.

**Core Consistent Strategy:**  
The agent operates by pursuing contiguous progress within the environment through a sequence of locally adjacent or aligned cell selections—prioritizing uninterrupted advancement in a consistent spatial direction (vertical or horizontal) or incremental cluster-building. When movement along the current direction becomes constrained by boundaries, obstacles, or structural limits, the agent switches to target special-object cells (e.g., those with ‘advance_token’), which serve as a progression or termination condition. Actions never target inaccessible, blocked, or isolated open regions; the strategy is grounded in stepwise expansion along available, adjacent cells or immediate engagement with goal-signaling objects once standard progression halts.

**Context-Dependent Behaviors:**  
The preferred direction of movement (vertical, horizontal, cluster-adjacent) and the specific sequence of targeted cells vary depending on local grid structure and obstacle placement. The agent may proceed straight upwards, downwards, leftwards, or act adjacent to clusters, contingent on the spatial configuration and accessibility of paths or openings in each environment instance. The location and timing of interaction with special tokens are determined by their emergence within accessible areas once routine path advancement is exhausted.

**Discarded Explanations:**  
There is no evidence of random exploration, detours around obstacles, or direct movement into distant open areas disconnected from current progression. The agent does not attempt moves into blocked cells, revisit previously completed regions, or select open regions that are non-adjacent to its current trajectory or cluster.

**Unified Environment-Level Strategy:**  
The agent systematically advances through available, adjacent cells—preferring continuous sequential or cluster-based movement along open paths within the environment. This process continues until progression is blocked or the structure of the environment shifts, at which point the agent transitions to interact with designated special-object cells to achieve advancement or conclude its objective. The overarching strategy is local expansion that adapts to environmental layout but always culminates in purposeful engagement with critical tokens or cells when forward progress in the current mode is no longer viable.
--

## Output Format Constraint

Return ONLY the Python code for the policy, wrapped in a Markdown fenced code block
that starts with ```python and ends with ```.

The code must define a function with the following signature:

```python
def policy(obs):
    ...
    return action
```

Where `action` is:
- a tuple (row, col)

You may define helper functions inside the code.
Do not use external libraries.
Do not include explanations, comments, or markdown.
Return only executable Python code.
Do not include any text outside the code block.

--

## Notes on StopTheFall Assumptions

- No StopTheFall-specific helper functions are provided.
- The policy must infer the agent and object locations directly from the grid observation.
- Falling, red, static, and advance cells are treated as observable cues.
- The policy does not have access to the environment’s transition dynamics.
- All movement, collision, and failure semantics are defined by the environment.
