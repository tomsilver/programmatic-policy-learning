## System Prompt
You are an expert programmer and problem solver.
Your task is to write a correct and general policy in Python.
You may define helper functions if needed.
Return only executable Python code.

--

## Environment description:

- The environment is a grid world of the game 'ReachForTheStar'.
- The observation `obs` is a 2D Python NumPy array (rows × columns).
  - Do not use boolean checks such as `if obs`, `if not obs`, or `obs == []`. 
- Each cell contains exactly one of the following symbols:

  - 'empty'
  - 'agent'
  - 'star'
  - 'drawn'
  - 'left_arrow'
  - 'right_arrow'

## Game Dynamics and Rules

- The game takes place on a 2D grid representing a vertical navigation environment.
- There are two key entities:
  - **Agent**: the controllable character.
  - **Star**: a fixed goal location that the agent must reach.
- The agent occupies exactly one grid cell at all times.
- The star occupies exactly one grid cell and does not move.
- The environment may include structural elements (represented implicitly via `drawn` cells) that affect movement feasibility.

### Movement and Interaction Mechanics

- On each step, the agent selects a target cell `(row, col)` as its action.
- The environment interprets the selected cell and determines whether and how the agent moves.
- Movement is constrained by the environment’s internal rules, which may include gravity-like effects, support requirements, or blocking structures.
- Selecting an unreachable or invalid cell results in no movement.
- The agent cannot move arbitrarily in all directions at all times; movement feasibility depends on the surrounding grid configuration.

### Directional Cues

- Arrow cells (`left_arrow`, `right_arrow`) appear in the grid and serve as **implicit directional guidance**.
- These arrows do not constitute actions themselves but correlate with movement directions that are often viable or successful in the current configuration.
- The agent may use the spatial arrangement of arrow cells as hints about which movements are likely to succeed.

### Objective and Termination

- The objective is to guide the agent to the star’s location.
- The episode ends successfully when the agent reaches the star cell.
- The episode continues indefinitely until success or until externally terminated by the environment.
- There are no explicit penalties or rewards provided to the policy other than episode termination.

### Interaction Constraints and Implications

- The policy does not have access to the environment’s physics, gravity rules, or movement constraints.
- The policy must infer feasible actions purely from the spatial layout of the grid.
- Progress toward the star often requires:
  - Horizontal positioning before vertical ascent,
  - Utilizing stable structures beneath the agent,
  - Respecting the environment’s implicit movement constraints.
- All legality checks, movement execution, and termination conditions are handled internally by the environment.


--

## Action space:

The policy must return a tuple (row, column) corresponding to a cell in the grid.

- (row, column) must be a valid cell location within the grid bounds.
- Selecting a cell represents choosing that cell as the agent’s action.
- The environment defines how selecting a cell affects the agent’s movement.
- Selecting an invalid or unreachable cell has no effect.


--

## Task Description
Task:
Guide the agent to reach the star cell.
The episode ends successfully when the agent reaches the star.

--

## Expert-Derived Hints

Below is a consolidated description of the agent’s demonstrated decision-making strategy, inferred from multiple expert trajectories. The statements summarize consistent behaviors observed across rollouts and are intended to guide policy synthesis, not to prescribe explicit rules or primitives.

**Core Consistent Strategy:**  
The agent reliably postpones movement until a continuous, marked path (using special symbols, such as '*') has been constructed from its starting position to the goal (the star). Throughout this preparatory phase, the agent stays in place and interactively extends or completes a traversable route by targeting specific empty or designated grid cells, ensuring that a connected, safe corridor is established. Only once this path is complete does the agent initiate movement, progressing strictly along the route of marked cells, step by step, toward the goal. Movement never occurs onto unmarked or non-designated cells, and detours or shortcuts are systematically avoided. The agent’s ability to act is strictly conditional: without a fully constructed path, it will not begin movement, regardless of proximity or open spaces.

**Context-Dependent Behaviors:**  
The specific direction in which the path is constructed (e.g., horizontally, vertically, or diagonally) and the choice of which cues to respond to (such as selecting arrows pointing left or right) depend on the initial arrangement of the environment and the location of interactive objects. In some contexts, when multiple directional cues (arrows) are available, the agent consistently follows their guidance as part of the constructed route; in others, path construction alone suffices. The agent adapts the shape and extension of the path—turning, branching, or changing direction as necessitated by the spatial configuration of obstacles and boundaries—to ensure continuous progress toward the goal, but always resumes its core strategy of route construction and dedicated traversal. Movements may involve horizontal, vertical, or diagonal advances, dictated by the established path and environmental layout.

**Unified Environment-Level Description:**  
The agent operates via a two-phase process: first, it ensures that a continuous, marked pathway exists from its current location to the goal by systematically selecting and designating cells along the intended route; second, it follows this constructed path exactly, moving only along marked cells and strictly refraining from deviating or initiating movement before the path is complete. Movement decisions and path-construction choices are always governed by the need to connect the agent and the goal via a continuous, safe route, with environmental cues (such as arrows) incorporated when they form part of the constructed path. The strategy is one of deliberate, adaptive route engineering followed by precise, rule-bound traversal.

--

## Output Format Constraint

Return ONLY the Python code for the policy, wrapped in a Markdown fenced code block
that starts with ```python and ends with ```.

The code must define a function with the following signature:

```python
def policy(obs):
    ...
    return action
```

Where `action` is:
- a tuple (row, col)

You may define helper functions inside the code.
Do not use external libraries.
Do not include explanations, comments, or markdown.
Return only executable Python code.
Do not include any text outside the code block.

--

## Notes on ReachForTheStar Assumptions 

- No ReachForTheStar-specific helper functions are provided.
- The policy must infer the agent and target locations directly from the grid observation.
- Arrow cells are treated as directional cues but do not correspond to explicit actions.
- The policy does not have access to the environment’s transition dynamics.
- All movement and interaction semantics are defined by the environment.
