## System Prompt
You are an expert programmer and problem solver.
Your task is to write a correct and general policy in Python.
You may define helper functions if needed.
Return only executable Python code.

--

## Environment description:

- The environment is a grid-based implementation of the game 'Chase'.
- The observation `obs` is a 2D Python NumPy array (rows × columns).
  - Do not use boolean checks such as `if obs`, `if not obs`, or `obs == []`. 
- Each cell contains one of the following values:
  - 'empty'
  - 'target'
  - 'agent'
  - 'wall'
  - 'drawn'
  - 'left_arrow'
  - 'right_arrow'
  - 'up_arrow'
  - 'down_arrow'


## Game Dynamics and Rules

- The game is played on a fixed-size 2D grid.
- Each grid cell contains exactly one symbol representing its contents:
  - agent: the controllable player character
  - target: the entity the agent must reach
  - wall (trees): impassable obstacles
  - empty: free space the agent can occup
  - drawn: special terrain (treated as non-blocking unless otherwise specified)
  - arrow cells: visual directional cues present in the grid but not actions themselves

### Movement and Actions

- At each time step, the agent chooses one action.
- An action is represented by selecting a single grid cell (row, col).
- Internally, the environment interprets this selection as a directional move (up, down, left, or right), based on the chosen cell.
- The agent attempts to move one cell in the corresponding direction.

### Movement Constraints

- If the destination cell in the chosen direction is:
  - empty or target → the agent moves into that cell.
  - wall or outside the grid → the agent does not move (the action has no effect).
- The agent cannot move diagonally.
- The agent cannot pass through walls or obstacles.

### Episode Termination and Success

- The episode terminates successfully when the agent occupies the same cell as the target.
- If the agent never reaches the target, the episode ends after a fixed maximum number of steps.
- Only the agent is controlled; the target does not respond to the agent’s actions unless defined by the environment.

### Observability and Knowledge

- The policy observes the entire grid state at each step.
- The policy does not have access to:
  - the environment’s transition function,
  - future states,
  - reward values,
  - or internal game logic beyond what can be inferred from observations.
- All decisions must be based solely on the current grid observation.

### Objective Summary

- The sole objective is to guide the agent through free cells, navigating around obstacles, to reach the target cell as efficiently as possible.

--

## Action Space

The policy must return a tuple (row, col) corresponding to a cell in the grid.

- The selected cell must be within the grid bounds.
- Selecting a cell represents choosing that cell as the agent’s action.
- The environment defines how selecting a cell affects the agent’s movement.
- Selecting a cell corresponding to a wall or invalid location has no effect.

--

## Task Description
Task:
Guide the agent to reach the target cell.
The episode ends successfully when the agent reaches the target.

--

## Expert-Derived Hints

Below is a consolidated description of the agent’s demonstrated decision-making strategy, inferred from multiple expert trajectories. The statements summarize consistent behaviors observed across rollouts and are intended to guide policy synthesis, not to prescribe explicit rules or primitives.

**Core Consistent Strategy:**  
The agent consistently seeks to minimize its spatial distance to the target by selecting movement actions that directly approach the target's position, prioritizing progress along one axis (typically horizontal alignment first, followed by vertical alignment) when not blocked. The agent exclusively uses environment-provided movement controls (such as directional arrows) and only acts when a move both reduces the distance to the target and does not require traversing an impassable cell (such as a wall). When no such move is available, the agent remains stationary, demonstrating patience rather than attempting detours or exploratory actions. The agent does not attempt illegal moves or bypass obstacles and waits until a viable, direct action is possible before proceeding step by step along the most direct open path.

**Context-Dependent Behaviors:**  
The agent’s movement direction (left/right or up/down) is contingent on the relative positions of itself and the target. The agent’s decision to move or remain stationary depends on both the presence of obstacles and the availability of movement controls aligned with a clear path toward the target. In scenarios where movement in the optimal direction is blocked by a wall or unavailable through the current controls, the agent refrains from acting, regardless of the target’s behavior or movement.

**Discarded Explanations:**  
There is no evidence that the agent explores alternate routes, attempts to circumvent obstacles via indirect paths, or reacts to the target’s motion beyond maintaining the direct pursuit. The agent does not engage in exploratory, detour, or random actions, nor does it attempt illegal or futile moves into walls. Explanations suggesting opportunistic, adaptive, or learning behavior beyond the described direct, distance-minimizing pursuit are not supported.

**Unified Environment-Level Strategy:**  
The agent implements a strategy of direct pursuit: it minimizes its distance to the target by sequentially moving along the open, unobstructed axis—typically aligning with the target’s column first, then its row—using the environment’s directional controls. The agent only initiates movement when the action is both valid (not blocked by a wall or impassable cell) and reduces its distance to the target. When the environment presents no immediately available, legal move that decreases this distance, the agent remains stationary and waits for such an opportunity. This disciplined, non-exploratory approach reflects a consistent, environment-level policy of pursuing the most direct open path to the target, progressing only when legitimate, advantageous moves exist.

--

## Output Format Constraint

Return ONLY the Python code for the policy, wrapped in a Markdown fenced code block
that starts with ```python and ends with ```.

The code must define a function with the following signature:

```python
def policy(obs):
    ...
    return action
```

Where `action` is:
- a tuple (row, col)

You may define helper functions inside the code.
Do not use external libraries.
Do not include explanations, comments, or markdown.
Return only executable Python code.
Do not include any text outside the code block.

--

## Notes on Chase Assumptions

- No Chase-specific helper functions are provided.
- The policy must infer the agent and target positions directly from the grid observation.
- Arrow cells are treated as directional cues but do not correspond to explicit actions.
- The policy does not have access to the environment’s transition function.
- All movement and interaction semantics are defined by the environment and must not be reimplemented by the policy.
