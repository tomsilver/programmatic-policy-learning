## System Prompt
You are an expert programmer and problem solver.
Your task is to write a correct and general policy in Python.
You may define helper functions if needed.
Return only executable Python code.

--

## Environment description:

- The environment is a grid-based implementation of the game 'Chase'.
- The observation `obs` is a 2D Python NumPy array (rows × columns).
  - Do not use boolean checks such as `if obs`, `if not obs`, or `obs == []`. 
- Each cell contains one of the following values:
  - 'empty'
  - 'target'
  - 'agent'
  - 'wall'
  - 'drawn'
  - 'left_arrow'
  - 'right_arrow'
  - 'up_arrow'
  - 'down_arrow'


## Game Dynamics and Rules

- The game is played on a fixed-size 2D grid.
- Each grid cell contains exactly one symbol representing its contents:
  - agent: the controllable player character
  - target: the entity the agent must reach
  - wall (trees): impassable obstacles
  - empty: free space the agent can occup
  - drawn: special terrain (treated as non-blocking unless otherwise specified)
  - arrow cells: visual directional cues present in the grid but not actions themselves

### Movement and Actions

- At each time step, the agent chooses one action.
- An action is represented by selecting a single grid cell (row, col).
- Internally, the environment interprets this selection as a directional move (up, down, left, or right), based on the chosen cell.
- The agent attempts to move one cell in the corresponding direction.

### Movement Constraints

- If the destination cell in the chosen direction is:
  - empty or target → the agent moves into that cell.
  - wall or outside the grid → the agent does not move (the action has no effect).
- The agent cannot move diagonally.
- The agent cannot pass through walls or obstacles.

### Episode Termination and Success

- The episode terminates successfully when the agent occupies the same cell as the target.
- If the agent never reaches the target, the episode ends after a fixed maximum number of steps.
- Only the agent is controlled; the target does not respond to the agent’s actions unless defined by the environment.

### Observability and Knowledge

- The policy observes the entire grid state at each step.
- The policy does not have access to:
  - the environment’s transition function,
  - future states,
  - reward values,
  - or internal game logic beyond what can be inferred from observations.
- All decisions must be based solely on the current grid observation.

### Objective Summary

- The sole objective is to guide the agent through free cells, navigating around obstacles, to reach the target cell as efficiently as possible.

--

## Action Space

The policy must return a tuple (row, col) corresponding to a cell in the grid.

- The selected cell must be within the grid bounds.
- Selecting a cell represents choosing that cell as the agent’s action.
- The environment defines how selecting a cell affects the agent’s movement.
- Selecting a cell corresponding to a wall or invalid location has no effect.

## Action Interpretation (Click Semantics)

In this environment, actions are expressed by selecting (clicking) a cell in the grid.
The meaning of a click is environment-dependent and is interpreted by the environment,
not by the policy itself.

The policy does NOT directly specify movement directions, rule logic, or outcomes.
It only selects a grid cell, and the environment determines the resulting effect.

### Click Semantics (Chase)

- Clicking a cell containing a directional arrow
  ('left_arrow', 'right_arrow', 'up_arrow', 'down_arrow')
  attempts to move the agent one step in that direction.
- Clicking any other cell has no effect.
- The agent may remain stationary if no valid directional click is available.
- The policy does not control the target; the target’s movement is external.

--

## Task Description
Task:
Guide the agent to reach the target cell.
The episode ends successfully when the agent reaches the target.

--

## Expert-Derived Hints

Below is a consolidated description of the agent’s demonstrated decision-making strategy, inferred from multiple expert trajectories. The statements summarize consistent behaviors observed across rollouts and are intended to guide policy synthesis, not to prescribe explicit rules or primitives.

{}

--

## Output Format Constraint

Return ONLY the Python code for the policy, wrapped in a Markdown fenced code block
that starts with ```python and ends with ```.

The code must define a function with the following signature:

```python
def policy(obs):
    ...
    return action
```

Where `action` is:
- a tuple (row, col)

You may define helper functions inside the code.
Do not use external libraries.
Do not include explanations, comments, or markdown.
Return only executable Python code.
Do not include any text outside the code block.

--

## Notes on Chase Assumptions

- No Chase-specific helper functions are provided.
- The policy must infer the agent and target positions directly from the grid observation.
- Arrow cells are treated as directional cues but do not correspond to explicit actions.
- The policy does not have access to the environment’s transition function.
- All movement and interaction semantics are defined by the environment and must not be reimplemented by the policy.
